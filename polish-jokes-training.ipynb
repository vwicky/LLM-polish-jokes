{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11509664,"sourceType":"datasetVersion","datasetId":7216954},{"sourceId":11509996,"sourceType":"datasetVersion","datasetId":7217220},{"sourceId":235373643,"sourceType":"kernelVersion"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import load_from_disk\nfrom peft import LoraConfig, get_peft_model\nimport torch\nimport numpy as np\nfrom trl import SFTTrainer\n\n# Add the necessary global before loading the checkpoint\ntorch.serialization.add_safe_globals([np.core.multiarray._reconstruct])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T16:15:53.128595Z","iopub.execute_input":"2025-04-22T16:15:53.129238Z"}},"outputs":[{"name":"stderr","text":"2025-04-22 16:16:02.982306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745338563.202853      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745338563.262077      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Load tokenizer and model\nmodel_name = \"flax-community/papuGaPT2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model without quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    weights_only=False\n).to(\"cuda\")  # Directly load on GPU","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add LoRA adapters\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"c_attn\", \"c_proj\"],  # GPT2-specific\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, lora_config)\nmodel = model.to(\"cuda\")\nprint(f\"Model moved to device: {next(model.parameters()).device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load tokenized dataset\ndataset = load_from_disk(\"/kaggle/input/polish-jokes-tokenised-polish-jokes/tokenized_polish_jokes\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainerCallback\nimport random\n\nclass JokeLoggerCallback(TrainerCallback):\n    def __init__(self, tokenizer, prompt_list, log_every=500, max_new_tokens=40):\n        self.tokenizer = tokenizer\n        self.prompt_list = prompt_list\n        self.log_every = log_every\n        self.max_new_tokens = max_new_tokens\n\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step % self.log_every == 0 and state.global_step != 0:\n            model = kwargs[\"model\"]\n            prompt = random.choice(self.prompt_list)\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=self.max_new_tokens,\n                    do_sample=True,\n                    top_k=50,\n                    top_p=0.95,\n                    temperature=1.0\n                )\n            print(f\"\\n--- Sample joke @ step {state.global_step} ---\")\n            print(self.tokenizer.decode(outputs[0], skip_special_tokens=True))\n            print(\"-\" * 50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./qlora_output\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=16,\n    logging_steps=10,\n    save_steps=500,\n    save_total_limit=5,\n    num_train_epochs=18,\n    learning_rate=7e-5,\n    fp16=True,\n    report_to=\"none\",\n    save_strategy=\"steps\",\n    resume_from_checkpoint=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=False\n)\n\n# Define sample prompts (Polish joke starters)\nsample_prompts = [\n    \"Przychodzi baba do lekarza i mówi\",\n    \"Dlaczego blondynka weszła do sklepu\",\n    \"Jasiu pyta nauczycielkę\",\n    \"Facet wchodzi do baru i widzi\"\n]\n\ncallbacks = [JokeLoggerCallback(tokenizer, sample_prompts, log_every=150)]","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    data_collator=data_collator,\n    callbacks=callbacks\n)\n\n# Sanity check\nprint(\"Trainer device:\", training_args.device)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Current device:\", torch.cuda.current_device())\nprint(\"Device name:\", torch.cuda.get_device_name(0))\n\n# Train\ntrainer.train()\n\n# Save\ntrainer.save_model(\"polish-joke-gpt-lora\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}